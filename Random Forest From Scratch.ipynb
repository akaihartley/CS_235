{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36fa9775",
   "metadata": {},
   "source": [
    "# Random Forest From Scratch\n",
    "\n",
    "## CS235: Data Mining Techniques\n",
    "### Dan O'Connor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be597f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06cbe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and changr from dataframes to arrays for algo compatability #use arrays instead of pd.series, dfs\n",
    "X_train_pca = pd.read_csv('X_train_pca.csv').to_numpy()\n",
    "X_test_pca = pd.read_csv('X_test_pca.csv').to_numpy()\n",
    "y_train = pd.read_csv('y_train.csv').to_numpy()\n",
    "y_test = pd.read_csv('y_test.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9919497f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirm\n",
    "type(X_train_pca), type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd0d0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa28b2ba",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb9e6a",
   "metadata": {},
   "source": [
    "Here I will explain the idea behind a decision tree and my coding approach. \n",
    "I broke up creating the random forest classifier into three main sections. To make code run smoother and interact easier I decided to do object oriented by creating classes. A random forest is combination of multiple decision trees which are themselves a combination of multiple decision nodes. I took a bottom up approach and begin with nodes, then decision trees, then finally random forest. Below I will give description of the classes, their methods, and how they interact with each other to create a random forest classifier.\n",
    "\n",
    "\n",
    "\n",
    "##### Node Class\n",
    "This class contains no methods.\n",
    "    \n",
    "For each point in the decision tree where a decision is made, this class is utilized. This class has no methods, but rather is useful to store information about the feature being considered for the split, the threshold for splitting data, and any information gain from making that split. Each node can have a left and right child, representing the branches of decisions. When a node doesn't need to make a decision, it becomes a 'leaf node' holding a prediction value instead.\n",
    "\n",
    "##### Decision Tree Classifier Class\n",
    "1. Initialization(`__init__`):\n",
    "\n",
    "- Sets up the maximum depth of the tree (max_depth), default is None\n",
    "- Initializes the tree's root node as None\n",
    "\n",
    "2. Fitting the Model (`fit`):\n",
    "\n",
    "- Starting point for the decision tree\n",
    "- Initializes the root by calling the `build_tree` method with the training data\n",
    "- The `build_tree` method is called with a starting depth of 0 and set to the root node\n",
    "\n",
    "3. Building the Tree (`build_tree`):\n",
    "\n",
    "- First checks if the stopping conditions are met, such as reaching the maximum depth or arriving at a leaf node (a node with no further splits). If so, it returns a leaf node.\n",
    "- If the stopping conditions are not met, it finds the best split using  `find_best_split` \n",
    "    - `find_best_split`  iteratively checks each feature and its possible threshold values to determine the most informative split, using the `information_gain`\n",
    "        - `information_gain` calculates info gain of potential split using weighted children, which measures the purity of a split using `gini`\n",
    "            - `gini` is used to calculate the gini impurity of a given subset of data\n",
    "- Once the best split is found, `build_tree` creates a new node with left and right branches, corresponding to the split data\n",
    "- Recursive process, building out the whole tree\n",
    "\n",
    "4. Predicting Output (`predict`):\n",
    "\n",
    "- Make predictions on unseen data (test set)\n",
    "- Traverses the tree for each data point in the input X, following the path determined by the splits (based on feature values) until it reaches a leaf node\n",
    "- Prediction for each data point is the value of the leaf node reached at the end of this path, determined by `calc_leaf_value`\n",
    "    - `calc_leaf_value`: returns the most common class within the leaf node\n",
    "\n",
    "\n",
    "##### Random Forest  Classifier Class\n",
    "\n",
    "1. Initialization (`__init__`):\n",
    "\n",
    "- Sets the number of decision trees in the forest (n_estimators), default is 100\n",
    "- Defines the maximum depth for each tree (max_depth), default is None\n",
    "- Initializes an empty list to store the trees\n",
    "- max_features is set to None inititially\n",
    "\n",
    "\n",
    "2. Fitting the Model (`fit`):\n",
    "\n",
    "- Initializes the trees list as empty at the start of fitting\n",
    "- Determines the number of features to consider when looking for the best split (max_features) as the square root of the total number of features\n",
    "- For each of the n_estimators:\n",
    "    - Creates a new instance of `DecisionTreeClassifier`\n",
    "    - Generates a bootstrap sample of the training data using the bootstrap_sample method\n",
    "    - Fits the decision tree to this bootstrap sample\n",
    "    - Appends the fitted tree to the trees list\n",
    "           \n",
    "3. Bootstrap Sampling (`bootstrap_sample`):\n",
    "\n",
    "- Randomly samples the indices of the training data (X) with replacement\n",
    "- The size of the bootstrap sample is equal to the size of the original training data.\n",
    "\n",
    "4. Predicting Output (`predict`):\n",
    "\n",
    "- For each data point in the input X, each tree in the forest makes a prediction.\n",
    "- Returns the majority vote class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28013cc1",
   "metadata": {},
   "source": [
    "### Node Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55cf8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "    Node in DT.\n",
    "    \n",
    "    Feat_idx: feature index of the node\n",
    "    Threshold: the threshold value of split for that feature\n",
    "    left: left child\n",
    "    right: right child\n",
    "    leaf_value: the leaf value\n",
    "    info_gain: the information gain at the node\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, feat_idx=None, threshold=None, left=None, right=None, leaf_value=None, info_gain=None):\n",
    "        #for decision nodes\n",
    "        self.feature_idx = feat_idx\n",
    "        self.threshold = threshold\n",
    "        self.left = left \n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        \n",
    "        #for leaf node\n",
    "        self.leaf_value = leaf_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca496b6",
   "metadata": {},
   "source": [
    "### Decision Tree Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2374b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \n",
    "    def __init__(self, max_depth=None, max_features=None):\n",
    "        #initialze max depth\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        #initializing the root as `None`\n",
    "        self.root = None\n",
    "        \n",
    "    def information_gain(self, y, l_y, r_y):\n",
    "        ''' \n",
    "        function to compute information gain, using the class values (y)\n",
    "        Determines weighted purity of split at node\n",
    "        \n",
    "        '''\n",
    "        #weighted child weights\n",
    "        weight_l = len(l_y) / len(y)\n",
    "        weight_r = len(r_y) / len(y)\n",
    "        \n",
    "        #Gini information gain\n",
    "        info_gain = self.gini(y) - (weight_l*self.gini(l_y) + weight_r*self.gini(r_y))    \n",
    "        return info_gain\n",
    "    \n",
    "    \n",
    "    def gini(self, y):\n",
    "        #gini formula, where m is the number of classes. In this case, only binary classification but kept formula general.\n",
    "        m = len(y)\n",
    "        return 1 - sum(((np.sum(y == c))/ m) ** 2 for c in np.unique(y))\n",
    "    \n",
    "    def calc_leaf_value(self, y):\n",
    "        #returns most frequent label on leaf\n",
    "        return np.argmax(np.bincount(y))\n",
    "    \n",
    "    def find_best_split(self, X, y):\n",
    "\n",
    "        #brute force finding the best splits, going through all the features and possible thresholds\n",
    "        best_split = {} #dictionary to store the splits\n",
    "        max_info = -np.inf #maximum possible, will act as placeholder for first iteration\n",
    "        \n",
    "        \n",
    "        n_feats = X.shape[1]\n",
    "        \n",
    "        if self.max_features is not None:\n",
    "            features = np.random.choice(range(n_feats), size=self.max_features, replace=False)\n",
    "        else:\n",
    "            features = range(n_feats)\n",
    "        #iterate through features\n",
    "        for feat_idx in features:\n",
    "            feat_values = X[:,feat_idx] #all of the values for that feature \n",
    "            poss_thresh = np.unique(feat_values) #all possible thresholds (all unique values of that feature)\n",
    "            \n",
    "            #iterate through thresholds to find information gain (how good of split)\n",
    "            for threshold in poss_thresh:\n",
    "                \n",
    "                #split the data, using feature index and threshold value\n",
    "                left_x, right_x, left_y, right_y = self.split_data(X, y, feat_idx, threshold)\n",
    "                \n",
    "                #ensuring there is data on each child\n",
    "                if (len(left_y) > 0) and (len(right_y)> 0):\n",
    "                    \n",
    "                    #              \n",
    "                    current_info = self.information_gain(y, left_y, right_y)\n",
    "                    \n",
    "                    #if the current info of iteration is greater, update dictionary (better split)\n",
    "                    if current_info > max_info:\n",
    "                        best_split['feat_idx'] = feat_idx\n",
    "                        best_split['threshold'] = threshold\n",
    "                        best_split['right_X'] = right_x\n",
    "                        best_split['left_X'] = left_x\n",
    "                        best_split['right_y'] = right_y\n",
    "                        best_split['left_y'] = left_y\n",
    "                        best_split['info_gain'] = current_info\n",
    "                        max_info = current_info\n",
    "                \n",
    "        return best_split\n",
    "                \n",
    "    def split_data(self, X, y, feat_idx, threshold):\n",
    "        '''\n",
    "        Splits the data\n",
    "        \n",
    "        '''\n",
    "        #split the data by threshold\n",
    "        left_mask = X[:, feat_idx] <= threshold\n",
    "        right_mask = X[:, feat_idx] > threshold\n",
    "\n",
    "        #X data\n",
    "        left_data_X = X[left_mask]\n",
    "        right_data_X = X[right_mask]\n",
    "        \n",
    "        #proper structure w/ ravel for y data\n",
    "        left_data_y = np.ravel(y[left_mask]) \n",
    "        right_data_y = np.ravel(y[right_mask]) \n",
    "\n",
    "        return left_data_X, right_data_X, left_data_y, right_data_y\n",
    "\n",
    "    def build_tree(self, X, y, current_depth=0):\n",
    "\n",
    "        #Stopping condition. If the depth reaches max depth (when specified) OR a child is pure, return the leaf node.\n",
    "        if len(np.unique(y)) == 1 or (self.max_depth is not None and current_depth >= self.max_depth):\n",
    "            return Node(leaf_value=self.calc_leaf_value(y))\n",
    "        \n",
    "        #find the best split \n",
    "        best_split = self.find_best_split(X, y)\n",
    "        \n",
    "        #if the dictionary exists and information gain is greater than 0, continue recursively\n",
    "        if best_split and best_split['info_gain'] > 0:\n",
    "            \n",
    "            #recursively build l and r branches of tree\n",
    "            left_branch = self.build_tree(best_split['left_X'], best_split['left_y'], current_depth + 1)\n",
    "            right_branch = self.build_tree(best_split['right_X'], best_split['right_y'], current_depth + 1)\n",
    "\n",
    "            #return node w/ branches attached to it\n",
    "            return Node(\n",
    "                feat_idx=best_split['feat_idx'], \n",
    "                threshold=best_split['threshold'], \n",
    "                left=left_branch, \n",
    "                right=right_branch,\n",
    "                info_gain=best_split['info_gain']\n",
    "            )\n",
    "\n",
    "        #if no split is found, return leaf node and calculate majority.\n",
    "        return Node(leaf_value=self.calc_leaf_value(y))\n",
    "    \n",
    "    def predict(self, X):\n",
    "    \n",
    "        \n",
    "        #empty list to store predictions\n",
    "        predictions = []\n",
    "        \n",
    "        #iterate thru each data point\n",
    "        for inputs in X:\n",
    "            #begin form root\n",
    "            node = self.root\n",
    "            \n",
    "            #Traverse until leaf is reach\n",
    "            while node.left is not None or node.right is not None:\n",
    "                \n",
    "                #decide left or right based on node threshold\n",
    "                if inputs[node.feature_idx] <= node.threshold:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "                    \n",
    "            #once node is reached, append leaf value to pred        \n",
    "            predictions.append(node.leaf_value)\n",
    "        return predictions\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #fit the DT\n",
    "        self.root = self.build_tree(X, y, current_depth=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9be07d",
   "metadata": {},
   "source": [
    "Checking if it functions as expected on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8a48c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "#lets check this on a known dataset where 100% acc is feasible\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "#convert to binary classification\n",
    "y_binary = np.where(y == 0, 0, 1)\n",
    "\n",
    "#train samples\n",
    "X_sample = X[:100]\n",
    "y_sample = y_binary[:100]\n",
    "\n",
    "#dt defined above\n",
    "dt = RandomForestClassifier()\n",
    "dt.fit(X_sample, y_sample)\n",
    "\n",
    "#preds\n",
    "predictions = dt.predict(X_sample)\n",
    "accuracy = accuracy_score(y_sample, predictions)\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a5fea5",
   "metadata": {},
   "source": [
    "### Random Forest Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ed68ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=100, max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = None\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        \n",
    "        #sqrt max feaets\n",
    "        self.max_features = int(np.sqrt(X.shape[1]))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, max_features=self.max_features)\n",
    "            X_sample, y_sample = self.bootstrap_sample(X, y)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def bootstrap_sample(self, X, y):\n",
    "        #num rows\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        #random sample of indices with replacement. Size is equal to size of original\n",
    "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        \n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        tree_preds = np.swapaxes(tree_preds, 0, 1)  #swap axes \n",
    "        majority_votes = np.array([np.argmax(np.bincount(tree_pred)) for tree_pred in tree_preds])\n",
    "        \n",
    "        return majority_votes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d98d8b0",
   "metadata": {},
   "source": [
    "## Test on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9740c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to sample the data to do a comprehensive search with much smaller size to reduce computation time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_sample, _, y_sample, _ = train_test_split( X_train_pca, y_train, stratify=y_train, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6918b303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4800, 15), (4800, 1))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample.shape, y_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2068700",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SMOTE to 'balance' training  dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_pca, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de6af42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37382, 15), (37382,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_smote.shape, y_train_smote.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68682e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_SMOTE, _, y_sample_SMOTE, _ = train_test_split( X_train_smote, y_train_smote, stratify=y_train_smote,\\\n",
    "                                                        test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be3bea41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4485, 15), (4485,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample_SMOTE.shape, y_sample_SMOTE.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e8120",
   "metadata": {},
   "source": [
    "From previous work on finding best parameters with SK-learn RF. Will impliment what I can with my model. Can control max_depth and n_estimators\n",
    "\n",
    "Optimized RF: Best parameters: {'criterion': 'gini', 'max_depth': 6, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
    "\n",
    "SMOTE optimized RF: Best parameters: {'criterion': 'gini', 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 110}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80699f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.95      0.87      4673\n",
      "           1       0.50      0.19      0.28      1327\n",
      "\n",
      "    accuracy                           0.78      6000\n",
      "   macro avg       0.65      0.57      0.57      6000\n",
      "weighted avg       0.74      0.78      0.74      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=6, n_estimators=100)\n",
    "rf.fit(X_sample, y_sample)\n",
    "y_pred= rf.predict(X_test_pca)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3135c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84      4673\n",
      "           1       0.46      0.60      0.52      1327\n",
      "\n",
      "    accuracy                           0.76      6000\n",
      "   macro avg       0.67      0.70      0.68      6000\n",
      "weighted avg       0.78      0.76      0.77      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_smote = RandomForestClassifier(max_depth=12, n_estimators=110)\n",
    "rf_smote.fit(X_sample_SMOTE, y_sample_SMOTE)\n",
    "y_pred_SMOTE = rf_smote.predict(X_test_pca)\n",
    "class_report_SMOTE = classification_report(y_test,y_pred_SMOTE)\n",
    "print(class_report_SMOTE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
