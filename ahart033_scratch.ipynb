{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Resources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4482, 15), (4482,), (5604, 15), (5604,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train_pca = pd.read_csv('X_train_pca.csv')\n",
    "X_test_pca = pd.read_csv('X_test_pca.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "y_test = pd.read_csv('y_test.csv')\n",
    "y_train, y_test = y_train.values.ravel(), y_test.values.ravel()\n",
    "y_train, y_test = np.where(y_train <= 0, -1, 1), np.where(y_test <= 0, -1, 1)\n",
    "X_train_pca, X_test_pca = X_train_pca.to_numpy(), X_test_pca.to_numpy()\n",
    "X_sample, _, y_sample, _ = train_test_split(X_train_pca, y_train, test_size=0.80, stratify=y_train, random_state=1)\n",
    "X_sample.shape, y_sample.shape, X_test_pca.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 15), (2000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# used for short run time\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "sampling_strategy = {1: 1000, -1: 1000}\n",
    "rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=1)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train_pca, y_train)\n",
    "X_resampled.shape, y_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM implementation using SMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Followed the paper: https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf\n",
    "class SVM_SMO:\n",
    "    def __init__(self, C=100, tol=0.001, class_weight = False, max_iteration=100, tau=1e-6, gamma=0.1, kernel_type='rbf'):\n",
    "        self.C = C # C - Regularization parameter\n",
    "        self.tol = tol # tol - Tolerance for stopping criterion\n",
    "        self.class_weight = class_weight # class_weight - Flag to use class weights (True/False)\n",
    "        self.max_iteration = max_iteration # max_iteration - Maximum number of iterations for the SMO algorithm\n",
    "        self.tau = tau # tau - Tolerance to avoid division by zero in kernel calculations\n",
    "        self.gamma = gamma # gamma - Parameter for RBF kernel\n",
    "        self.kernel_type = kernel_type # kernel_type - Type of kernel ('linear' or 'rbf')\n",
    "        self.alpha = None\n",
    "        self.b = None\n",
    "        self.X_T = None\n",
    "        self.y_T = None\n",
    "\n",
    "    # Radial Basis Function kernel\n",
    "    def rbf_kernel(self, x1, x2):\n",
    "        return np.exp(-self.gamma * np.linalg.norm(x1 - x2) ** 2)\n",
    "\n",
    "    # Linear kernel\n",
    "    def linear_kernel(self, x1, x2):\n",
    "        return np.dot(x1, x2)\n",
    "\n",
    "    # Compute the kernel matrix for the training data\n",
    "    def compute_kernel_matrix(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        self.kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "        # Compute the kernel function for each pair of points using the specified kernel type\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                if self.kernel_type == 'linear':\n",
    "                    self.kernel_matrix[i, j] = self.linear_kernel(X[i], X[j])\n",
    "                else:\n",
    "                    self.kernel_matrix[i, j] = self.rbf_kernel(X[i], X[j])\n",
    "\n",
    "    # Compute the gradient of the objective function with respect to alpha\n",
    "    def compute_gradient(self):\n",
    "        gradient = self.kernel_matrix @ (self.alpha * self.y_T) + self.y_T\n",
    "        return gradient\n",
    "\n",
    "    # Select the working set of alpha pairs to update\n",
    "    def select_working_set(self):\n",
    "        gradient = self.compute_gradient()\n",
    "        # Compute the indices of the alpha pairs that violate the KKT conditions\n",
    "        I_up = (self.y_T == 1) & (self.alpha < self.C) | (self.y_T == -1) & (self.alpha > 0)\n",
    "        I_low = (self.y_T == -1) & (self.alpha < self.C) | (self.y_T == 1) & (self.alpha > 0)\n",
    "\n",
    "        # Compute the indices of the alpha pairs that violate the KKT conditions and have the largest and smallest gradients\n",
    "        a = self.kernel_matrix + self.kernel_matrix.T - 2 * np.diag(self.kernel_matrix.diagonal())\n",
    "        b = -self.y_T * gradient\n",
    "\n",
    "        # Set the diagonal of a to tau to avoid division by zero\n",
    "        a[a <= 0] = self.tau\n",
    "\n",
    "        # Compute the indices of the alpha pairs that violate the KKT conditions and have the largest and smallest gradients\n",
    "        i = np.argmax(np.where(I_up, -self.y_T * gradient, -np.inf))\n",
    "        j = np.argmin(np.where(I_low, b[i]**2 / a[i], np.inf))\n",
    "\n",
    "        return i, j\n",
    "\n",
    "    def update_alpha(self, i, j):\n",
    "        # Extract entries from the kernel matrix\n",
    "        Q_i = self.kernel_matrix[i, :]\n",
    "        Q_j = self.kernel_matrix[j, :]\n",
    "\n",
    "        # Calculate the quadratic coefficient\n",
    "        quad_coef = max((Q_i[i] + Q_j[j] - 2 * Q_i[j]), self.tau)\n",
    "\n",
    "        # Compute delta, the amount to change alpha by\n",
    "        delta = (-self.y_T[i] + self.y_T[j]) / quad_coef \n",
    "        diff = self.alpha[i] - self.alpha[j]\n",
    "\n",
    "        # Add class weights to the alpha values\n",
    "        weight_i = self.class_weights[self.y_T[i]]\n",
    "        weight_j = self.class_weights[self.y_T[j]]\n",
    "\n",
    "        # Update alpha[i] and alpha[j] \n",
    "        self.alpha[i] += delta * weight_i\n",
    "        self.alpha[j] -= delta * weight_j\n",
    "\n",
    "        # Clip alpha values based on their difference and the value of C\n",
    "        if diff > 0:\n",
    "            if self.alpha[j] < 0:\n",
    "                self.alpha[j] = 0\n",
    "                self.alpha[i] = diff\n",
    "        else:\n",
    "            if self.alpha[i] < 0:\n",
    "                self.alpha[i] = 0\n",
    "                self.alpha[j] = -diff\n",
    "\n",
    "        if diff > self.C - i - self.C - j:\n",
    "            if self.alpha[i] > self.C - i:\n",
    "                self.alpha[i] = self.C - i\n",
    "                self.alpha[j] = self.C - i - diff\n",
    "        else:\n",
    "            if self.alpha[j] > self.C - j:\n",
    "                self.alpha[j] = self.C - j\n",
    "                self.alpha[i] = self.C - j + diff\n",
    "    \n",
    "    # Check if the stopping criteria is met\n",
    "    def stopping_criteria_met(self, gradient):\n",
    "        return np.all(np.logical_or(\n",
    "            np.logical_and(self.alpha <= 0, gradient >= -self.tol),\n",
    "            np.logical_and(self.alpha >= self.C, gradient <= self.tol)\n",
    "        ))\n",
    "    \n",
    "    # Compute the class weights\n",
    "    def compute_weights(self, y):\n",
    "        n_samples = len(y)\n",
    "        # y needs adjustment as the class labels are -1 and 1\n",
    "        adjusted_y = (y + 1) // 2\n",
    "        class_counts = np.bincount(adjusted_y, minlength=2)\n",
    "        class_counts = np.maximum(class_counts, 1)\n",
    "        n_classes = 2\n",
    "        class_weights = n_samples / (n_classes * class_counts)\n",
    "        return class_weights\n",
    "\n",
    "    # Fit the SVM model\n",
    "    def fit(self, X, y):\n",
    "        self.X_T = X\n",
    "        self.y_T = y\n",
    "        self.alpha = np.zeros(len(y))\n",
    "        # Begin with computing the kernel matrix\n",
    "        self.compute_kernel_matrix(X)\n",
    "\n",
    "        # Compute class weights if True, set to 1 otherwise\n",
    "        if self.class_weight == True:\n",
    "            self.class_weights = self.compute_weights(y)\n",
    "        else:\n",
    "            self.class_weights = {label: 1 for label in np.unique(y)}\n",
    "\n",
    "        # SMO algorithm\n",
    "        iteration = 0\n",
    "        while iteration < self.max_iteration:\n",
    "            i, j = self.select_working_set()\n",
    "            self.update_alpha(i, j)\n",
    "            gradient = self.compute_gradient()\n",
    "            if self.stopping_criteria_met(gradient):\n",
    "                break\n",
    "            iteration += 1\n",
    "        \n",
    "        # indices of support vectors\n",
    "        support_vectors = [i for i in range(len(y)) if 0 < self.alpha[i] < self.C] \n",
    "        # Compute the bias term\n",
    "        if support_vectors:\n",
    "            self.b = np.mean([y[i] - np.dot(self.alpha * y, self.kernel_matrix[i])] for i in support_vectors)\n",
    "        else: \n",
    "            self.b = 0 # default bias\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predict class labels for the given test data\n",
    "        kernel_applied = np.array([\n",
    "            [self.linear_kernel(x, x_train) if self.kernel_type == 'linear' else self.rbf_kernel(x, x_train)\n",
    "            for x_train in self.X_T] for x in X\n",
    "        ])\n",
    "\n",
    "        # Decision function calculation\n",
    "        y_pred = np.dot(kernel_applied, self.alpha * self.y_T) + self.b\n",
    "        return np.sign(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6473947180585297\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.86      0.65      0.74      4341\n",
      "           1       0.35      0.65      0.45      1263\n",
      "\n",
      "    accuracy                           0.65      5604\n",
      "   macro avg       0.61      0.65      0.60      5604\n",
      "weighted avg       0.75      0.65      0.68      5604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = SVM_SMO(C=1, tol=0.01, class_weight= False, max_iteration = 100, kernel_type = 'linear')\n",
    "svc.fit(X_sample, y_sample)\n",
    "y_pred = svc.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
